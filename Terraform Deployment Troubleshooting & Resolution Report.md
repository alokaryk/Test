#Terraform Deployment Troubleshooting & Resolution Report (Proxmox K8s Cluster)Goal: Deploy a 7-node Kubernetes cluster (1 Bastion, 3 Control Plane, 3 Worker) on Proxmox VE 8.4.0 using Terraform, ensuring sequential VM creation and correct provider syntax.Status: RESOLVED (terraform apply successful after applying final syntax fixes).Terraform Host: 192.168.1.1001. Phase 1: Provider Compatibility ConflictsThe initial challenge was determining the correct, compatible Terraform provider for Proxmox VE 8.4.0, as the community providers frequently change syntax and maintenance status.ProblemSymptoms & ErrorsRoot CauseResolutionP1: Deprecated Provider (Initial)Configuration used source = "telmate/proxmox" and resource proxmox_vm_qemu.The telmate/proxmox provider is deprecated and incompatible with modern Proxmox (8.x) and required nested blocks.Action: Switched to the actively maintained community provider, bpg/proxmox, and updated the required_providers block and resource type to proxmox_vm_qemu (later confirmed proxmox_virtual_environment_vm).P2: Registry Lookup FailureError: Failed to query available provider packages for bpg/proxmoxve.The specific provider name bpg/proxmoxve was retired/renamed on the Terraform Registry, or the local cache was corrupted.Action: Cleared the local cache (rm -rf .terraform), and switched the source name to the currently active bpg/proxmox (without the ve suffix).P3: Provider Argument NamingError: Unsupported argument on pm_api_url, pm_user, etc.The new bpg/proxmox provider uses modern, explicit attribute names, while the configuration used legacy telmate prefixes (pm_api_url).Action: Updated the provider "proxmox" block attributes to the correct names (endpoint, username, password, insecure).2. Phase 2: Configuration Syntax ErrorsThe strict HCL parsing rules of the bpg/proxmox provider rejected several common Terraform patterns.ProblemSymptoms & ErrorsRoot CauseResolutionP4: Cloud-Init Block StructureError: Unsupported block type on initialization, ci_public_keys, ipconfig, etc.The provider requires a specific, multi-layered nesting for Cloud-Init settings, rejecting both the telmate flat structure and simplified modern structures.Action: Enforced the fully correct, multi-line nested block structure (initialization { ip_config { ipv4 { ... } } }) across all VM definitions.P5: Inline HCL SyntaxError: Invalid character (on ;) and Argument definition required.Attempted to use compressed, single-line HCL blocks (e.g., cpu { cores = 1; type = "host" }), which is invalid in this provider.Action: Manually expanded all compressed blocks (e.g., cpu, clone, initialization) to the strict, verbose multi-line HCL format.P6: VM Resource NamingError: Invalid resource type on proxmox_vm_qemu.The final provider (bpg/proxmox) uses the modern resource name for VMs.Action: Changed all VM resource names to proxmox_virtual_environment_vm.3. Phase 3: Infrastructure Logic & ProvisioningProblemSymptoms & ErrorsRoot CauseResolutionP7: API Connection Timeout (HTTP 596)Error: error waiting for VM clone: ... received an HTTP 596 response - Reason: Connection timed out.Proxmox experienced high I/O load trying to clone and start 7 large VMs simultaneously, causing the API request from the Terraform Host to time out.Action: Removed the for_each loop for VM creation and implemented a sequential deployment strategy by breaking the VM creation into 7 individual blocks (k8s_control_01, k8s_control_02, etc.) and linking them with explicit depends_on clauses.P8: Provisioner LogicError: Unsupported argument on proxy_command and only_if.The provisioning blocks were initially incorrectly placed inside the proxmox_virtual_environment_vm resources, and the null_resource used the wrong connection syntax.Action: Moved all provisioning logic into dedicated null_resource blocks and updated the connection blocks to use the correct modern arguments (bastion_host, bastion_user).ConclusionThe final, successful configuration utilizes the stable bpg/proxmox provider and addresses the infrastructure's resource limitations by forcing sequential deployment.The deployment will proceed in the following fixed order:proxmox_virtual_environment_vm.bastion (VMID 401)proxmox_virtual_environment_vm.k8s_control_01 (VMID 402)...proxmox_virtual_environment_vm.k8s_worker_03 (VMID 407)null_resource.k8s_init (Initializes K8s on Control-01)null_resource.k8s_join_nodes_controls (Joins C02, C03)null_resource.k8s_join_nodes_workers (Joins W01, W02, W03)The cluster is now ready for the final configuration steps: CNI installation (Flannel) and NFS CSI deployment using Helm.
